<!DOCTYPE html>

<html>
<head>
<title>connection between PCA and linear regression</title>
<link href="https://cdn.sstatic.net/Shared/stacks.css?v=079c5e1603be" rel="stylesheet" type="text/css"/>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript"> </script>
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.0/css/bootstrap.min.css" rel="stylesheet"/>
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.0/css/bootstrap-theme.min.css" rel="stylesheet"/>
<style>
        .row {
          display: block;
          margin-left: auto;
          margin-right: auto;
          width:50%;
        }
        tr {
          border:1px solid lightgrey;
        }
        </style>
</head>
<body>
<div>
<div class="row" id="question-title">
<h1> connection between PCA and linear regression </h1>
<hr/>
</div>
<div class="row">
<div class="question">
<div id="question" question_id="684336">
<p>Is there a formal link between linear regression and PCA? The goal of PCA is to decompose a matrix into a linear combination of variables that contain most of the information in the matrix. Suppose for sake of argument that we're doing PCA on an input matrix rather than its covariance matrix, and the columns <span class="math-container" id="6873252" visual_id="5611238"><math alttext="X_{1},X2,...,X_{n}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><mrow><mi>X</mi><mo>⁢</mo><mn>2</mn></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>X</mi><mi>n</mi></msub></mrow></semantics></math></span> of the matrix are variables of interest. Then intuitively it seems that the PCA procedure is similar to a linear regression where one uses a linear combination of the variables to predict the entries in the matrix. Is this correct thinking? How can it be made mathematically precise?</p> <p>Imagine enumerating the (infinite) space of all linear combinations of the variables <span class="math-container" id="6873253" visual_id="1474"><math alttext="X_{1},X_{2},...,X_{n}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><msub><mi>X</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>X</mi><mi>n</mi></msub></mrow></semantics></math></span> of a matrix of data and doing linear regression on each such combination to measure how much of the rows of the matrix the combination can 'explain'. Is there an interpretation of what PCA doing in terms of this operation? I.e. how in this procedure PCA would select the 'best' linear combinations? I realize this procedure is obviously not computationally feasible, I only present it to try to make the link between PCA and linear regression. This procedure works directly with linear combinations of columns of a matrix so it does not require them to be orthogonal.</p>
</div>
<hr/>
<div id="tags">
<span> linear-algebra </span><span> matrices </span><span> statistics </span><span> regression </span><span> statistical-inference </span>
</div>
<hr/>
<div id="question-comments">
<table>
<tbody>
<tr><td comment_id="1455744"> Hm, PCA and OLS are not the same---the two models are different in a basic way. But people do do PCA on the regressors before running a linear regression. This can help de-correlate the regressors---that's what the PCA is designed to to---and reduce standard errors. But formally that seems a little iffy to me, as PCA assumes a multivariate normal distribution. </td></tr>
</tbody>
</table>
</div>
</div>
<hr style="border-top: 3px double #8c8b8b"/>
</div>
<div class="row">
<div class="answer">
<div answer_id="717589" id="answer">
<p>The difference between PCA and regression can be interpreted as being <em>mathematically</em> only one additional multiplication with an inverse matrix... </p> <p>Here is a correlation matrix with three groups of variables:        </p> <pre><code>group g1: <span class="math-container" id="6873254" visual_id="5611239"><math alttext="x_{1,1},x_{1,2}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow><mn>1</mn><mo>,</mo><mn>2</mn></mrow></msub></mrow></semantics></math></span> correlated          group g2: <span class="math-container" id="6873255" visual_id="5611240"><math alttext="x_{2,1},x_{2,2},x_{2,3}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow><mn>2</mn><mo>,</mo><mn>2</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow><mn>2</mn><mo>,</mo><mn>3</mn></mrow></msub></mrow></semantics></math></span> correlated          group g3: <span class="math-container" id="6873256" visual_id="68"><math alttext="y" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi></semantics></math></span> composed by the variables of group g1 and g2          </code></pre> <p>The correlation matrix looks like</p> <pre><code>cor         x1_1    x1_2    x2_1    x2_2    x2_3    ___y ----------------------------------------------------------- x1_1        1.000   0.976   0.472   0.444   0.430   0.642 x1_2        0.976   1.000   0.594   0.567   0.538   0.767 x2_1        0.472   0.594   1.000   0.998   0.979   0.919 x2_2        0.444   0.567   0.998   1.000   0.986   0.917 x2_3        0.430   0.538   0.979   0.986   1.000   0.904 ___y        0.642   0.767   0.919   0.917   0.904   1.000 ----------------------------------------------------------- </code></pre> <p>This is the loadings-matrix in its initial triangular cholesky form when not yet rotated to PC-position:     </p> <pre><code>tri         f1       f2      f3      f4      f5      f6 ----------------------------------------------------------- x1_1        1.000   0.000   0.000   0.000   0.000   0.000 x1_2        0.976   0.218   0.000   0.000   0.000   0.000 x2_1        0.472   0.612   0.634   0.000   0.000   0.000 x2_2        0.444   0.615   0.649   0.054   0.000   0.000 x2_3        0.430   0.543   0.700   0.119   0.128   0.000 ___y        0.642   0.644   0.350   0.156   0.112   0.117 ----------------------------------------------------------- </code></pre> <p>In pca we do not distinguish between independent and dependent variables; so we might do a rotation to PCA-position, where then the first axis/column denotes the first principal component and so on.         </p> <pre><code>[22] pca = rot(dr,"pca")  pca         f1        f2      f3      f4      f5      f6 ----------------------------------------------------------- x1_1        0.714   -0.692   0.100   0.041  -0.020   0.005 x1_2        0.810   -0.584  -0.031  -0.047   0.025  -0.007 x2_1        0.948    0.300   0.063  -0.081   0.003   0.018 x2_2        0.940    0.333   0.050  -0.049  -0.015  -0.019 x2_3        0.926    0.351   0.072   0.114   0.016  -0.001 ___y        0.973    0.045  -0.226   0.027  -0.010   0.004 ----------------------------------------------------------- </code></pre> <p>We see, that all variables have one common factor, but we might also see, that only two or three factors are "relevant".  A quartimax-rotation might locate the three main factors better related to the variable-groups        </p> <pre><code>[26] qua = rot(pca,"quartimax",1..6,1..3)  qua.max       f1      f2      f3      f4      f5      f6 -----------------------------------+------------------------ x1_1        0.373   0.925   -0.060   0.041  -0.020   0.005 x1_2        0.505   0.859    0.068  -0.047   0.025  -0.007 -------- x2_1        0.988   0.112   -0.059  -0.081   0.003   0.018 x2_2        0.994   0.078   -0.048  -0.049  -0.015  -0.019 x2_3        0.989   0.056   -0.071   0.114   0.016  -0.001 -------- ___y        0.908   0.342    0.240   0.027  -0.010   0.004 -----------------------------------+------------------------ </code></pre> <p>We see that we have two main groups with high in-group correlations, saying they measure nearly the same, in the x-variables and a less sharp separated "group" with only the y-variable which is correlated to both groups but has still an individual variance (in factor f3).<br/> This is classical PCA with Quartimax/Varimax-rotation, the "little jiffy"-procedure.              </p> <p>Now we move on to regression. In regression we define one variable as dependent, in our case the variable y. We are interested, in which way y is composed by the independent variables; a still pca-inherent point of view would be that we find the pca of the independent variables only and leve the factor f6, which shows a part of y-variance which is uncorrelated , alone as taken from the initial triangular cholesky-factor.     </p> <pre><code>[29] pca = rot(dr,"pca",1..5)  pca.reg      f1       f2      f3      f4      f5      f6 ---------------------------------------------------+-------- x1_1        0.729   -0.680   0.067  -0.048  -0.002  0.000 x1_2        0.816   -0.571  -0.067   0.055   0.001  0.000 ------------------- x2_1        0.946    0.315  -0.066  -0.033   0.019  0.000 x2_2        0.936    0.348  -0.041  -0.013  -0.023  0.000 x2_3        0.923    0.366   0.117   0.036   0.004  0.000 ---------------------------------------------------+-------- ___y        0.957    0.057  -0.076   0.245  -0.039  0.117 ---------------------------------------------------+-------- </code></pre> <p>Still the axes show the "factors" and how each variable is composed by that common (f1 to f5) or individual (f6) factors.    </p> <p>Regression asks now for composition of y not by the factors/coordinates on the axes but by the coordinates on variables x if they are taken as axes.<br/> Happily we need only multiply the current loadings-matrix by the inverse of the x-submatrix to get axes defined by the x and have the "loadings" of y on x:        </p> <pre><code>[32] B = (pca[*,1..5]*inv(pca[1..5,1..5]) ) || pca[*,6]  reg.B       x1_1    x1_2    x2_1    x2_2    x2_3    ___y ---------------------------------------------------+-------- x1_1        1.000   0.000   0.000   0.000   0.000   0.000 x1_2        0.000   1.000   0.000   0.000   0.000   0.000 ------------------- x2_1        0.000   0.000   1.000   0.000   0.000   0.000 x2_2        0.000   0.000   0.000   1.000   0.000   0.000 x2_3        0.000   0.000   0.000   0.000   1.000   0.000 ---------------------------------------------------+-------- ___y        -1.442  1.989   -1.394  0.955   0.876   0.117 </code></pre> <p>We see, that each of the axes is identified with one of the variables and also the "loadings" of y on that axes. But because the axes show now the directions of the x the loadings of y in the last row are now also the "regression"-weights/ coefficients, and the regression weights are now           </p> <pre><code>B = [-1.442  1.989   -1.394  0.955   0.876 ] </code></pre> <p><em>(Because of the strong correlations in the groups the regression-weights are above 1 and also the signs are alternating. But this is not much of concern here in that methodic explanation)</em> </p> <p><hr/> <strong><em>[Update]</em></strong> <hr/> Relating PCA and regression in this way, there occurs very fluently another instructive example which might improve intuition. This is the problem of multicollinearity, which if occurs in regression is <em>a problem</em> for the researcher, but if occurs in PCA only <em>improves the validity</em> of estimation of separate components and the loadings of the items on such (latent) constructs.<br/> The means, which I want to introduce here, is the "main direction" of the multicollinear items (which of course is the first principal component) respectively the two sets of independent items <span class="math-container" id="6873257" visual_id="113687"><math alttext="x1" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>⁢</mo><mn>1</mn></mrow></semantics></math></span> and <span class="math-container" id="6873258" visual_id="113688"><math alttext="x2" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>⁢</mo><mn>2</mn></mrow></semantics></math></span>. We can introduce latent variables which mark the pc's of the two sets of x-items. This can practically be done applying pc-rotation with maximizing criterion taken from the sets only:              </p> <pre><code>[32] pc1 = rot(tri,"pca",1..2,1..5)  //rotating for pc of items 1..2 only [33] pc1 = {pc1,marksp(pc1,{1,2})}   //add markers for that pc's    PC1       pc1_1     pc1_2     c3        c4        c5        c6 -------------------------------------------------------------- x1_1      0.994    -0.109     0         0         0         0 x1_2      0.994     0.109     0         0         0         0 -------------------------------------------------------------- x2_1      0.562     0.571     0.599     0         0         0 x2_2      0.536     0.578     0.613     0.054     0         0 x2_3      0.521     0.516     0.657     0.123     0.125     0 -------------------------------------------------------------- y         0.722     0.575     0.316     0.151     0.11      0.116 =============================================================== pc1_1     1         0         0         0         0         0 pc1_2     0         1         0         0         0         0 =============================================================== </code></pre> <p>If we were looking at the system of <span class="math-container" id="6873259" visual_id="5611241"><math alttext="x1_{1},x1_{2}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>x</mi><mo>⁢</mo><msub><mn>1</mn><mn>1</mn></msub></mrow><mo>,</mo><mrow><mi>x</mi><mo>⁢</mo><msub><mn>1</mn><mn>2</mn></msub></mrow></mrow></semantics></math></span> and <span class="math-container" id="6873260" visual_id="68"><math alttext="y" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi></semantics></math></span> only, we had already the beta-values for the pc's of that <span class="math-container" id="6873261" visual_id="113687"><math alttext="x1" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>⁢</mo><mn>1</mn></mrow></semantics></math></span>-item set as <span class="math-container" id="6873262" visual_id="5611242"><math alttext="b_{pc1_{1}}=0.722" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mrow><mi>p</mi><mo>⁢</mo><mi>c</mi><mo>⁢</mo><msub><mn>1</mn><mn>1</mn></msub></mrow></msub><mo>=</mo><mn>0.722</mn></mrow></semantics></math></span> and <span class="math-container" id="6873263" visual_id="5611243"><math alttext="b_{pc1_{2}}=0.575" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mrow><mi>p</mi><mo>⁢</mo><mi>c</mi><mo>⁢</mo><msub><mn>1</mn><mn>2</mn></msub></mrow></msub><mo>=</mo><mn>0.575</mn></mrow></semantics></math></span> - no hazzle because of (multi)collinearity!            </p> <p>The same can be done with the second set of independent items <span class="math-container" id="6873264" visual_id="5611244"><math alttext="x2_{1},x2_{2},x2_{3}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>x</mi><mo>⁢</mo><msub><mn>2</mn><mn>1</mn></msub></mrow><mo>,</mo><mrow><mi>x</mi><mo>⁢</mo><msub><mn>2</mn><mn>2</mn></msub></mrow><mo>,</mo><mrow><mi>x</mi><mo>⁢</mo><msub><mn>2</mn><mn>3</mn></msub></mrow></mrow></semantics></math></span>: </p> <pre><code>[37] pc2 = rot(pc1,"pca",3..5,1..5)                                    [38] pc2 = {pc2,marksp(pc2,{1,2,3})}                                 PC2       pc2_1     pc2_2     pc2_3     c4        c5        c6 -------------------------------------------------------------- x1_1      0.477    -0.124    -0.464     0.729     0.101     0 x1_2      0.599    -0.189    -0.389     0.636     0.221     0 -------------------------------------------------------------- x2_1      0.997    -0.078    -0.022     0         0         0 x2_2      0.999    -0.041     0.027     0         0         0 x2_3      0.993     0.119    -0.005     0         0         0 -------------------------------------------------------------- y         0.923    -0.029    -0.058     0.212     0.294     0.116 =============================================================== pc1_1     0.542    -0.157    -0.429     0.687     0.162     0 pc1_2     0.557    -0.300     0.343    -0.424     0.55      0 -------------------------------------------------------------- pc2_1     1         0         0         0         0         0 pc2_2     0         1         0         0         0         0 pc2_3     0         0         1         0         0         0 =============================================================== </code></pre> <p>The beta-value for the first pc of the second set of items (in a model without the first set) were <span class="math-container" id="6873265" visual_id="5611245"><math alttext="b_{pc2_{1}}=0.923" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mrow><mi>p</mi><mo>⁢</mo><mi>c</mi><mo>⁢</mo><msub><mn>2</mn><mn>1</mn></msub></mrow></msub><mo>=</mo><mn>0.923</mn></mrow></semantics></math></span> which is more than that <span class="math-container" id="6873266" visual_id="5611242"><math alttext="b_{pc1_{1}}=0.722" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mrow><mi>p</mi><mo>⁢</mo><mi>c</mi><mo>⁢</mo><msub><mn>1</mn><mn>1</mn></msub></mrow></msub><mo>=</mo><mn>0.722</mn></mrow></semantics></math></span> for the first pc of the first set of independent items.          </p> <p>Now to see the betas using the joint model requires again only the inversion of the submatrix of the loadings of the whole set of 5 pc-markers and postmultiplying the first 5 columns with that. This gives us the "loadings", if the 5 pcs are taken as axes of a coordinate system. We get     </p> <pre><code>[42] beta = pca1[*,1..5]* inv(pca1[7..11,1..5]) || pca1[*,6]          beta      pc1_1     pc1_2     pc2_1     pc2_2     pc2_3     c6 -------------------------------------------------------------- x1_1      0.994    -0.109     0         0         0         0 x1_2      0.994     0.109     0         0         0         0 -------------------------------------------------------------- x2_1      0         0         0.997    -0.078    -0.022     0 x2_2      0         0         0.999    -0.041     0.027     0 x2_3      0         0         0.993     0.119    -0.005     0 -------------------------------------------------------------- y         0.540     0.375     0.421     0.169     0.045     0.116 =============================================================== pc1_1     1         0         0         0         0         0 pc1_2     0         1         0         0         0         0 -------------------------------------------------------------- pc2_1     0         0         1         0         0         0 pc2_2     0         0         0         1         0         0 pc2_3     0         0         0         0         1         0 =============================================================== </code></pre> <p>In short:</p> <pre><code>beta      pc1_1     pc1_2  |   pc2_1     pc2_2     pc2_3   |  c6 ---------------------------+-------------------------------+-------- y         0.540     0.375  |   0.421     0.169     0.045   |  0.116 ===========================+===============================|======== </code></pre> <p>In that <strong><em>joint</em></strong> model the "main direction" of the first set of independents has a beta-weight of 0.540 and the "main direction" of the second set of 0.421 .  The value at <span class="math-container" id="6873267" visual_id="5611246"><math alttext="c6" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mo>⁢</mo><mn>6</mn></mrow></semantics></math></span> is here only for completeness: its square <span class="math-container" id="6873268" visual_id="5611247"><math alttext="0.116^{2}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mn>0.116</mn><mn>2</mn></msup></semantics></math></span> is the unexplained variance of the dependent item <span class="math-container" id="6873269" visual_id="68"><math alttext="y" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi></semantics></math></span>. </p>
</div>
<hr/>
<div id="answer-comments">
<table>
<tbody>
<tr><td comment_id="1903139"> can you relate your answer to @Eric Towers 's answer? </td></tr><tr><td comment_id="3739891"> @user9576 : well, after I have shown such a shortcut-relation between the two concepts, what shall I say other than that the introducing "No." is wrong? And that the singular decomposition is not the only way to compute PCA and this is also well known (Eigen-decomposition, Jacobi-method using Givens-rotations are two others).I've not followed the last link in Eric's answer, but possibly this is just, what I've exercised above (at least the link sounds like) </td></tr>
</tbody>
</table>
</div>
</div>
<hr style="border-top: 3px double #8c8b8b"/>
</div>
<div class="row">
<div id="duplicate">
<table>
<tbody>
</tbody>
</table>
</div>
<hr/>
<div id="related">
<table>
<tbody>
</tbody>
</table>
</div>
</div>
</div>
</body>
</html>
