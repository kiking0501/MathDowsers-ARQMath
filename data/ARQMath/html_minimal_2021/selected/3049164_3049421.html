<!DOCTYPE html>

<html>
<head>
<title>Can we obtain an explicit and efficient analytic interpolation of tetration by this method?</title>
<link href="https://cdn.sstatic.net/Shared/stacks.css?v=079c5e1603be" rel="stylesheet" type="text/css"/>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG.js" type="text/javascript"> </script>
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.0/css/bootstrap.min.css" rel="stylesheet"/>
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.0/css/bootstrap-theme.min.css" rel="stylesheet"/>
<style>
        .row {
          display: block;
          margin-left: auto;
          margin-right: auto;
          width:50%;
        }
        tr {
          border:1px solid lightgrey;
        }
        </style>
</head>
<body>
<div>
<div class="row" id="question-title">
<h1> Can we obtain an explicit and efficient analytic interpolation of tetration by this method? </h1>
<hr/>
</div>
<div class="row">
<div class="question">
<div id="question" question_id="3049164">
<p>I am curious about this. It has been a very long time since I have ever toyed with this topic but it was an old interest of mine quite some time ago - maybe 8 years (250 megaseconds) ago at least, and I never really got to a conclusion, nor do I think anyone did entirely satisfactorily, and some comments on a Youtube video I was watching inspired me to dust it off and try once more to give it another thwack.</p> <p>And the question is, basically, how can one construct a "reasonable" interpolation of the "tetration" operation, also called a "power tower", which for those who have not heard of it is defined for natural <span class="math-container" id="28231740" visual_id="663"><math alttext="n&amp;gt;1" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>n</mi><mo>&gt;</mo></mrow><mn>1</mn></mrow></semantics></math></span> and real <span class="math-container" id="28231741" visual_id="16031"><math alttext="a&amp;gt;1" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>a</mi><mo>&gt;</mo></mrow><mn>1</mn></mrow></semantics></math></span> by</p> <p><span class="math-container" id="28231742" visual_id="8201213"><math alttext="{}^{n}a=a\uparrow\uparrow n:=\underbrace{a^{a^{a^{...^{a}}}}}_{\mbox{$n$ % copies of $a$}}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mmultiscripts><mi>a</mi><mprescripts></mprescripts><none></none><mi>n</mi></mmultiscripts><mo>=</mo><mi>a</mi><mo>‚Üë</mo><mo>‚Üë</mo><mi>n</mi><mo>:=</mo><munder><munder accentunder="true"><msup><mi>a</mi><msup><mi>a</mi><msup><mi>a</mi><msup><mi mathvariant="normal">‚Ä¶</mi><mi>a</mi></msup></msup></msup></msup><mo movablelimits="false">‚èü</mo></munder><mrow><mi>n</mi><mtext>¬†copies of¬†</mtext><mi>a</mi></mrow></munder></mrow></semantics></math></span></p> <p>where the nested exponentiations on the left are evaluated in rightassociative fashion, so the deepest ("highest") layer is done first, e.g.</p> <p><span class="math-container" id="28231743" visual_id="8201214"><math alttext="{}^{3}3=3^{3^{3}}=3^{27}=7\ 625\ 597\ 484\ 987" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mmultiscripts><mn>3</mn><mprescripts></mprescripts><none></none><mn>3</mn></mmultiscripts><mo>=</mo><msup><mn>3</mn><msup><mn>3</mn><mn>3</mn></msup></msup><mo>=</mo><msup><mn>3</mn><mn>27</mn></msup><mo>=</mo><mn>7‚ÄÖ625‚ÄÖ597‚ÄÖ484‚ÄÖ987</mn></mrow></semantics></math></span></p> <p>and <em>not</em> left-associative, i.e.</p> <p><span class="math-container" id="28231744" visual_id="8201215"><math alttext="{}^{3}3\neq(3^{3})^{3}=3^{3\cdot 3}=3^{9}=19\ 683" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mmultiscripts><mn>3</mn><mprescripts></mprescripts><none></none><mn>3</mn></mmultiscripts><mo>‚â†</mo><msup><mrow><mo stretchy="false">(</mo><msup><mn>3</mn><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><mn>3</mn></msup><mo>=</mo><msup><mn>3</mn><mrow><mn>3</mn><mo>‚ãÖ</mo><mn>3</mn></mrow></msup><mo>=</mo><msup><mn>3</mn><mn>9</mn></msup><mo>=</mo><mn>19‚ÄÖ683</mn></mrow></semantics></math></span></p> <p>What the "interpolation" part means is basically, given that this definition clearly only <em>works for</em> values of the second argument, <span class="math-container" id="28231745" visual_id="177"><math alttext="n" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi></semantics></math></span> (called as such by analogy with exponentiation even though it is written first in the "left-superscript" notation just introduced), usually called the "height" of the tetration or power tower for obvious reasons, that are natural numbers at least 1, since we have to have a whole number of "copies of <span class="math-container" id="28231746" visual_id="159"><math alttext="a" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi></semantics></math></span>" - half a copy, say, wouldn't make much sense, as while you can literally write a half-written "<span class="math-container" id="28231747" visual_id="159"><math alttext="a" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi></semantics></math></span>", that is formally nonsense and has no mathematical meaning, although it may have other forms of meaning from other angles of human understanding and analysis, e.g. perhaps as a form of artsey commentary. Mathematical meaning is, though, of course, what we're interested in. We can, of course, naturally extend this in a way similar to extending exponentiation to the integers by noting that</p> <p><span class="math-container" id="28231748" visual_id="8201216"><math alttext="{}^{n+1}a=a^{{}^{n}a}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mmultiscripts><mi>a</mi><mprescripts></mprescripts><none></none><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mmultiscripts><mo>=</mo><msup><mi>a</mi><mmultiscripts><mi>a</mi><mprescripts></mprescripts><none></none><mi>n</mi></mmultiscripts></msup></mrow></semantics></math></span></p> <p>and thus</p> <p><span class="math-container" id="28231749" visual_id="8201217"><math alttext="{}^{n-1}a=\log_{a}(^{n}a)" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mmultiscripts><mi>a</mi><mprescripts></mprescripts><none></none><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></mmultiscripts><mo>=</mo><msub><mi>log</mi><mi>a</mi></msub><mrow><msup><mo stretchy="false">(</mo><mi>n</mi></msup><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow></semantics></math></span></p> <p>and if we do this we can at least extend that <span class="math-container" id="28231750" visual_id="8201218"><math alttext="{}^{0}a=1" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mmultiscripts><mi>a</mi><mprescripts></mprescripts><none></none><mn>0</mn></mmultiscripts><mo>=</mo><mn>1</mn></mrow></semantics></math></span>, similar to exponentiation, and <span class="math-container" id="28231751" visual_id="8201219"><math alttext="{}^{(-1)}a=0" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mmultiscripts><mi>a</mi><mprescripts></mprescripts><none></none><mrow><mo stretchy="false">(</mo><mrow><mo>-</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mmultiscripts><mo>=</mo><mn>0</mn></mrow></semantics></math></span>, a rather interesting result when viewed in contrast to exponentiation given that the first negative exponentiation of a number is not a constant but instead its reciprocal. Of course, we cannot extend now to <span class="math-container" id="28231752" visual_id="8201220"><math alttext="{}^{(-2)}a" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mmultiscripts><mi>a</mi><mprescripts></mprescripts><none></none><mrow><mo stretchy="false">(</mo><mrow><mo>-</mo><mn>2</mn></mrow><mo stretchy="false">)</mo></mrow></mmultiscripts></semantics></math></span>, as then we get <span class="math-container" id="28231753" visual_id="8201221"><math alttext="\log_{a}(0)" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>log</mi><mi>a</mi></msub><mo>‚Å°</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></mrow></semantics></math></span> which is undefined (though of course if you want to stretch the maths a bit and expand the codomain to the extended reals, you can say <span class="math-container" id="28231754" visual_id="8201222"><math alttext="{}^{(-2)}a=-\infty" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mmultiscripts><mi>a</mi><mprescripts></mprescripts><none></none><mrow><mo stretchy="false">(</mo><mrow><mo>-</mo><mn>2</mn></mrow><mo stretchy="false">)</mo></mrow></mmultiscripts><mo>=</mo><mrow><mo>-</mo><mi mathvariant="normal">‚àû</mi></mrow></mrow></semantics></math></span>. In any case though, <span class="math-container" id="28231755" visual_id="8201223"><math alttext="{}^{(-3)}a" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mmultiscripts><mi>a</mi><mprescripts></mprescripts><none></none><mrow><mo stretchy="false">(</mo><mrow><mo>-</mo><mn>3</mn></mrow><mo stretchy="false">)</mo></mrow></mmultiscripts></semantics></math></span> and further are definitely, <em>really</em> undefined, since no real exponential can be negative, much less negative infinity!). So this peters out.</p> <p>Of course, the most interesting bit - as hinted at with the "half a copy" business above - is trying to extend the height <span class="math-container" id="28231756" visual_id="177"><math alttext="n" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi></semantics></math></span> to <em>real</em> values, presumably in <span class="math-container" id="28231757" visual_id="1265607"><math alttext="(-2,\infty)" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mrow><mo>-</mo><mn>2</mn></mrow><mo>,</mo><mi mathvariant="normal">‚àû</mi><mo stretchy="false">)</mo></mrow></semantics></math></span> at least.</p> <p>And there have been a number of methods fielded in those past epochs which attempt to do this as well as some interesting ones regarding the conditions which are required to produce a suitably "natural" extension, given that it is trivially obvious that one can, of course, "interpolate" a given sparse sequence of points in any way that one desires and, moreover, even with the identities <span class="math-container" id="28231758" visual_id="8201216"><math alttext="{}^{n+1}a=a^{{}^{n}a}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mmultiscripts><mi>a</mi><mprescripts></mprescripts><none></none><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></mmultiscripts><mo>=</mo><msup><mi>a</mi><mmultiscripts><mi>a</mi><mprescripts></mprescripts><none></none><mi>n</mi></mmultiscripts></msup></mrow></semantics></math></span>, they only suffice to make it unique insofar as whole-number increments of the tower are concerned - fill any unit interval with anything you like, and the identity will extend to provide an interpolant that will satisfy it. For exponentiation, this non-uniqueness is much less of a problem because we also have the additional identity <span class="math-container" id="28231759" visual_id="438353"><math alttext="a^{n+m}=a^{n}a^{m}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mrow><mi>n</mi><mo>+</mo><mi>m</mi></mrow></msup><mo>=</mo><mrow><msup><mi>a</mi><mi>n</mi></msup><mo>‚Å¢</mo><msup><mi>a</mi><mi>m</mi></msup></mrow></mrow></semantics></math></span>, which lets us extend to rational values, however no such identity exists for tetration.</p> <p>In this regard, extension of tetration is similar to the question of extension of the factorial, which is similarly impoverished of identities, with new ones interestingly only coming about <em>after</em> the extension was done by Euler in the form of the gamma function, to meet a challenge originally proposed by Bernoulli to do exactly this. The gamma function, however, is still ostensibly more "natural" simply because a) it often crops up and b) it has some very cute integral representations, esp. the darlin'</p> <p><span class="math-container" id="28231760" visual_id="8201224"><math alttext="\Gamma(x)=\int_{0}^{1}[-\log(u)]^{n-1}\ du" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">Œì</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mo largeop="true" symmetric="true">‚à´</mo><mn>0</mn><mn>1</mn></msubsup><mrow><mpadded width="+5pt"><msup><mrow><mo stretchy="false">[</mo><mrow><mo>-</mo><mrow><mi>log</mi><mo>‚Å°</mo><mrow><mo stretchy="false">(</mo><mi>u</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">]</mo></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msup></mpadded><mo>‚Å¢</mo><mrow><mo rspace="0pt">ùëë</mo><mi>u</mi></mrow></mrow></mrow></mrow></semantics></math></span></p> <p>(Though with regard to objection a), one could say this may be simply because we have not yet <em>found</em> such an expression, and thus places where it might be useful, could be currently written off as "unsolvable".)</p> <p>Yet clearly, that doesn't seem to have been the case for tetration, either. Moreover, in all these past discussions, many of the extension methods proposed are in fact extremely cumbersome and computationally intensive to approximate, involving elaborate constructs like Riemann mappings, infinite limits of integral equations, and so forth - all things that are, while mathematically valid, both inelegant and also not something you're going be able to program into a software pack like Mathematica and have it spit out 2500 digits of <span class="math-container" id="28231761" visual_id="8201225"><math alttext="{}^{1/2}2" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mmultiscripts><mn>2</mn><mprescripts></mprescripts><none></none><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mmultiscripts></semantics></math></span> in the blink of an eye.</p> <p>But nonetheless, one particular method out of these proposed methods seems be both fairly simple and like that it might possible be amenable to more detailed analysis, and that is the "Carleman matrix" operator method.</p> <p>This method is most succinctly expressed for the specific case <span class="math-container" id="28231762" visual_id="60559"><math alttext="a=e" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>=</mo><mi>e</mi></mrow></semantics></math></span>, to construct the "natural tetrational" <span class="math-container" id="28231763" visual_id="8201226"><math alttext="\mathrm{tet}(x):=\ ^{x}e" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>tet</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><msup><mo rspace="7.5pt">:=</mo><mi>x</mi></msup><mi>e</mi></mrow></semantics></math></span> with real height <span class="math-container" id="28231764" visual_id="67"><math alttext="x" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi></semantics></math></span>, so we'll just focus on that for now. But basically it is based on the following two observations. The first is that one can consider the result of the height-<span class="math-container" id="28231765" visual_id="177"><math alttext="n" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi></semantics></math></span> power tower of <span class="math-container" id="28231766" visual_id="1073"><math alttext="e" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>e</mi></semantics></math></span> as the <em>iterated exponential</em> evaluated at <span class="math-container" id="28231767" visual_id="342"><math alttext="1" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1</mn></semantics></math></span>, namely</p> <p><span class="math-container" id="28231768" visual_id="8201227"><math alttext="{}^{n}e=\exp^{n}(1)" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mmultiscripts><mi>e</mi><mprescripts></mprescripts><none></none><mi>n</mi></mmultiscripts><mo>=</mo><mrow><msup><mi>exp</mi><mi>n</mi></msup><mo>‚Å°</mo><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mrow></mrow></semantics></math></span></p> <p>or perhaps more nicely for what we're about to do,</p> <p><span class="math-container" id="28231769" visual_id="8201228"><math alttext="{}^{n-1}e=\exp^{n}(0)" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mmultiscripts><mi>e</mi><mprescripts></mprescripts><none></none><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></mmultiscripts><mo>=</mo><mrow><msup><mi>exp</mi><mi>n</mi></msup><mo>‚Å°</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></mrow></mrow></semantics></math></span></p> <p>which has some interesting gamma-function like quality about it with the offset.</p> <p>And the second one is the following. If we let <span class="math-container" id="28231770" visual_id="14647"><math alttext="\exp" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>exp</mi></semantics></math></span> be given by its power series,</p> <p><span class="math-container" id="28231771" visual_id="160156"><math alttext="\exp(x)=\sum_{n=0}^{\infty}\frac{x^{n}}{n!}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>exp</mi><mo>‚Å°</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msubsup><mo largeop="true" symmetric="true">‚àë</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mi mathvariant="normal">‚àû</mi></msubsup><mfrac><msup><mi>x</mi><mi>n</mi></msup><mrow><mi>n</mi><mo>!</mo></mrow></mfrac></mrow></mrow></semantics></math></span></p> <p>then we can actually represent such iterated exponentials using what is called its <em>Carleman matrix</em>, basically the infinite-order "matrix" with entries</p> <p><span class="math-container" id="28231772" visual_id="8201229"><math alttext="C[\exp]_{ij}=\frac{i^{j}}{j!}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>C</mi><mo>‚Å¢</mo><msub><mrow><mo stretchy="false">[</mo><mi>exp</mi><mo stretchy="false">]</mo></mrow><mrow><mi>i</mi><mo>‚Å¢</mo><mi>j</mi></mrow></msub></mrow><mo>=</mo><mfrac><msup><mi>i</mi><mi>j</mi></msup><mrow><mi>j</mi><mo>!</mo></mrow></mfrac></mrow></semantics></math></span></p> <p>such that if we have the infinite vector of exponential coefficients <span class="math-container" id="28231773" visual_id="8201230"><math alttext="\mathbf{a}=\left[\frac{1}{1!}\ \frac{1}{2!}\ \frac{1}{3!}\ \cdots\right]^{T}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ùêö</mi><mo>=</mo><msup><mrow><mo>[</mo><mrow><mpadded width="+5pt"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>!</mo></mrow></mfrac></mpadded><mo>‚Å¢</mo><mpadded width="+5pt"><mfrac><mn>1</mn><mrow><mn>2</mn><mo>!</mo></mrow></mfrac></mpadded><mo>‚Å¢</mo><mpadded width="+5pt"><mfrac><mn>1</mn><mrow><mn>3</mn><mo>!</mo></mrow></mfrac></mpadded><mo>‚Å¢</mo><mi mathvariant="normal">‚ãØ</mi></mrow><mo>]</mo></mrow><mi>T</mi></msup></mrow></semantics></math></span> then the vector <span class="math-container" id="28231774" visual_id="8201231"><math alttext="\mathbf{b}_{n}=(\mathbf{C}[\exp])^{n}\mathbf{a}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ùêõ</mi><mi>n</mi></msub><mo>=</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mi>ùêÇ</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">[</mo><mi>exp</mi><mo stretchy="false">]</mo></mrow></mrow><mo stretchy="false">)</mo></mrow><mi>n</mi></msup><mo>‚Å¢</mo><mi>ùêö</mi></mrow></mrow></semantics></math></span> is the coefficients of <span class="math-container" id="28231775" visual_id="949251"><math alttext="\exp^{n}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>exp</mi><mi>n</mi></msup></semantics></math></span>. In particular, if we sum the top row, we get exactly what we want: <span class="math-container" id="28231776" visual_id="8201232"><math alttext="{}^{n-1}e" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mmultiscripts><mi>e</mi><mprescripts></mprescripts><none></none><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></mmultiscripts></semantics></math></span>.</p> <p>Now the question is, though, how can we compute this matrix power for <em>fractional</em> <span class="math-container" id="28231777" visual_id="177"><math alttext="n" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi></semantics></math></span> in some explicit form? It seems one possible way to do this, and the way that I saw when this method was suggested (by Gottfried Helms, who was here a long time ago, not sure if they're still so) was to try to diagonalize the matrix, so that you can use the fact that if a matrix <span class="math-container" id="28231778" visual_id="1684808"><math alttext="\mathbf{A}=\mathbf{P}\mathbf{D}\mathbf{P}^{-1}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ùêÄ</mi><mo>=</mo><msup><mi>ùêèùêÉùêè</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></semantics></math></span> for some diagonal matrix <span class="math-container" id="28231779" visual_id="16668"><math alttext="\mathbf{D}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùêÉ</mi></semantics></math></span> (which happens to be the matrix of eigenvalues) then <span class="math-container" id="28231780" visual_id="8201233"><math alttext="\mathbf{A}^{t}=\mathbf{P}\mathbf{D}^{t}\mathbf{P}^{-1}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ùêÄ</mi><mi>t</mi></msup><mo>=</mo><mrow><msup><mi>ùêèùêÉ</mi><mi>t</mi></msup><mo>‚Å¢</mo><msup><mi>ùêè</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mrow></semantics></math></span> where that the inner power is easy to compute as you just take exponents of the diagonal terms.</p> <p>And <em>numerically</em> this <em>seems</em> to work for at least finite truncations of the matrix <span class="math-container" id="28231781" visual_id="8201234"><math alttext="\mathbf{C}[\exp]" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ùêÇ</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">[</mo><mi>exp</mi><mo stretchy="false">]</mo></mrow></mrow></semantics></math></span>, but analytically it may be on shaky ground, as I see with this thread here that I just saw when trying to dig into this once more:</p> <p><a href="https://math.stackexchange.com/questions/2235730/diagonalization-of-an-infinite-matrix">Diagonalization of an infinite matrix</a></p> <p>and moreover it's still not <em>super</em> efficient as we'd like - we're not computing 2500 digits of <span class="math-container" id="28231782" visual_id="8201235"><math alttext="{}^{1/2}e" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mmultiscripts><mi>e</mi><mprescripts></mprescripts><none></none><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mmultiscripts></semantics></math></span> and I want them computed, dammit!</p> <p>However, it seems that in this case some of the objections raised in that thread do not perhaps apply here. In particular, it is mentioned how that an infinite matrix diagonalization is ambiguous (seems to mirror the situation with the tetration interpolation generally where there is great freedom) due to choice of suitable normed vector space over which to make sense of it, and moreover in that question it was pointed that the usual most "natural" space, <span class="math-container" id="28231783" visual_id="20791"><math alttext="\ell^{2}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi mathvariant="normal">‚Ñì</mi><mn>2</mn></msup></semantics></math></span>, did not work for <em>that questioner's</em> particular matrix, because in particular it would "map" most "vectors" of <span class="math-container" id="28231784" visual_id="20791"><math alttext="\ell^{2}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi mathvariant="normal">‚Ñì</mi><mn>2</mn></msup></semantics></math></span> to effectively outside of the space. <em>However</em>, this Carleman matrix seems better behaved - in particular, due to the fact that any vector <span class="math-container" id="28231785" visual_id="8201236"><math alttext="[\ a_{0}\ a_{1}\ a_{2}\ \cdots\ ]^{T}\in\ell^{2}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mo stretchy="false">[</mo><mrow><mpadded width="+5pt"><msub><mpadded lspace="5pt" width="+5pt"><mi>a</mi></mpadded><mn>0</mn></msub></mpadded><mo>‚Å¢</mo><mpadded width="+5pt"><msub><mi>a</mi><mn>1</mn></msub></mpadded><mo>‚Å¢</mo><mpadded width="+5pt"><msub><mi>a</mi><mn>2</mn></msub></mpadded><mo>‚Å¢</mo><mpadded width="+5pt"><mi mathvariant="normal">‚ãØ</mi></mpadded></mrow><mo stretchy="false">]</mo></mrow><mi>T</mi></msup><mo>‚àà</mo><msup><mi mathvariant="normal">‚Ñì</mi><mn>2</mn></msup></mrow></semantics></math></span> by definition must have terms that converge absolutely as an infinite sum, then that owing to the factorials in the matrix when we multiply by it we should also get an (even more) convergent series, as is illustrated by considering the bounding "vector" <span class="math-container" id="28231786" visual_id="8201237"><math alttext="[\ 1\ 1\ 1\ \cdots\ ]^{T}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="false">[</mo><mrow><mpadded width="+5pt"><mn>‚ÄÖ1‚ÄÖ1‚ÄÖ1</mn></mpadded><mo>‚Å¢</mo><mpadded width="+5pt"><mi mathvariant="normal">‚ãØ</mi></mpadded></mrow><mo stretchy="false">]</mo></mrow><mi>T</mi></msup></semantics></math></span> as "formally" acted upon by <span class="math-container" id="28231787" visual_id="8201234"><math alttext="\mathbf{C}[\exp]" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ùêÇ</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">[</mo><mi>exp</mi><mo stretchy="false">]</mo></mrow></mrow></semantics></math></span>. So it seems that in that regard, we are in better shape against at least the objections raised in that thread in this case than for that poster's scenario.</p> <p>Thus the question I have is, if we take the relevant target space as <span class="math-container" id="28231788" visual_id="20791"><math alttext="\ell^{2}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi mathvariant="normal">‚Ñì</mi><mn>2</mn></msup></semantics></math></span>, can we find a "natural" infinite diagonalization and thus matrix-power for this case, and moreover express it somehow in terms of at least some sort of infinite combinatorial sums or otherwise easily-manipulated expressions for its coefficients?</p> <p>Moreover, another interesting and seemingly natural question provoked by this is, exactly how sensitive <em>is</em> the method to the choice of at least norm used to interpret the matrix power, i.e. can we have absolute freedom to interpolate <span class="math-container" id="28231789" visual_id="8201238"><math alttext="{}^{n}e" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mmultiscripts><mi>e</mi><mprescripts></mprescripts><none></none><mi>n</mi></mmultiscripts></semantics></math></span> in any way we please, and if not, then just how much do we get? I suspect "a lot", but there are some "lots" that are more than others in mathematics, even when infinities are concerned, thanks to Cantor. And can we do the inverse - i.e. if I fill in <span class="math-container" id="28231790" visual_id="8201238"><math alttext="{}^{n}e" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mmultiscripts><mi>e</mi><mprescripts></mprescripts><none></none><mi>n</mi></mmultiscripts></semantics></math></span> with some freely-chosen interpolant in the interval <span class="math-container" id="28231791" visual_id="1356"><math alttext="[0,1]" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></semantics></math></span> (for the purpose of making things easy I'll assume it's continuous and moreover equals <span class="math-container" id="28231792" visual_id="342"><math alttext="1" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1</mn></semantics></math></span> at <span class="math-container" id="28231793" visual_id="2056"><math alttext="x=0" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow></semantics></math></span> and <span class="math-container" id="28231794" visual_id="1073"><math alttext="e" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>e</mi></semantics></math></span> at <span class="math-container" id="28231795" visual_id="1567"><math alttext="x=1" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow></semantics></math></span>) - can we find a norm such that the associated Carleman matrix power will produce that interpolant? Does it have to be analytic (seems right, but keep in mind that we are <em>summing the top row</em>, not necessarily creating a power series valid for all or even any inputs, though again it also "seems" right that if not analytic, it'll diverge)? If so, what's the proof?</p> <p>ADD (epoch time 1545.46 Ms): In the quest for an at least summatory-formula shot at the diagonalization, I note this matrix has the interesting relations among rows and columns given by</p> <p><span class="math-container" id="28231796" visual_id="8201239"><math alttext="C[\exp]_{(i+1)j}=\sum_{k=0}^{j}\frac{1}{(j-k)!}C_{ik}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>C</mi><mo>‚Å¢</mo><msub><mrow><mo stretchy="false">[</mo><mi>exp</mi><mo stretchy="false">]</mo></mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mo>‚Å¢</mo><mi>j</mi></mrow></msub></mrow><mo>=</mo><mrow><msubsup><mo largeop="true" symmetric="true">‚àë</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi>j</mi></msubsup><mrow><mfrac><mn>1</mn><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>j</mi><mo>-</mo><mi>k</mi></mrow><mo stretchy="false">)</mo></mrow><mo>!</mo></mrow></mfrac><mo>‚Å¢</mo><msub><mi>C</mi><mrow><mi>i</mi><mo>‚Å¢</mo><mi>k</mi></mrow></msub></mrow></mrow></mrow></semantics></math></span></p> <p>and</p> <p><span class="math-container" id="28231797" visual_id="8201240"><math alttext="C[\exp]_{i(j+1)}=\frac{i}{j+1}C_{ij}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>C</mi><mo>‚Å¢</mo><msub><mrow><mo stretchy="false">[</mo><mi>exp</mi><mo stretchy="false">]</mo></mrow><mrow><mi>i</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></msub></mrow><mo>=</mo><mrow><mfrac><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mo>‚Å¢</mo><msub><mi>C</mi><mrow><mi>i</mi><mo>‚Å¢</mo><mi>j</mi></mrow></msub></mrow></mrow></semantics></math></span></p> <p>Not sure if this helps anything, though. But at least it shows there is structure and thus we're not just dealing with effectively purely random matrices and thus in theory it might somehow be exploited in some fashion to simplify things.</p> <p>ADD 2 (same time): You should actually sum the second row of the matrix power to get the tetration <span class="math-container" id="28231798" visual_id="8201238"><math alttext="{}^{n}e" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mmultiscripts><mi>e</mi><mprescripts></mprescripts><none></none><mi>n</mi></mmultiscripts></semantics></math></span>, not the first to get <span class="math-container" id="28231799" visual_id="8201232"><math alttext="{}^{n-1}e" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mmultiscripts><mi>e</mi><mprescripts></mprescripts><none></none><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></mmultiscripts></semantics></math></span>. The first row always sums to 1.</p> <p>ADD 3 (ue+1545.47 Ms): The first row formula above allows us to derive the interesting property that if <span class="math-container" id="28231800" visual_id="8201234"><math alttext="\mathbf{C}[\exp]" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ùêÇ</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">[</mo><mi>exp</mi><mo stretchy="false">]</mo></mrow></mrow></semantics></math></span> is right-multiplied by the factorial matrix</p> <p><span class="math-container" id="28231801" visual_id="8201241"><math alttext="\mathbf{D}:=\begin{bmatrix}1&amp;amp;\frac{1}{1!}&amp;amp;\frac{1}{2!}&amp;amp;\frac{1}{3!% }&amp;amp;\cdots\\ 0&amp;amp;1&amp;amp;\frac{1}{1!}&amp;amp;\frac{1}{2!}&amp;amp;\cdots\\ 0&amp;amp;0&amp;amp;1&amp;amp;\frac{1}{1!}&amp;amp;\cdots\\ 0&amp;amp;0&amp;amp;0&amp;amp;1&amp;amp;\cdots\\ &amp;amp;&amp;amp;\cdots&amp;amp;&amp;amp;\end{bmatrix}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ùêÉ</mi><mo>:=</mo><mrow><mo>[</mo><mtable columnspacing="5pt" rowspacing="0pt"><mtr><mtd columnalign="center"><mn>1</mn></mtd><mtd columnalign="center"><mrow><mrow><mi>a</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>p</mi></mrow><mo>;</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>!</mo></mrow></mfrac></mrow></mtd><mtd columnalign="center"><mrow><mrow><mi>a</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>p</mi></mrow><mo>;</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mo>!</mo></mrow></mfrac></mrow></mtd><mtd columnalign="center"><mrow><mrow><mi>a</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>p</mi></mrow><mo>;</mo><mfrac><mn>1</mn><mrow><mn>3</mn><mo>!</mo></mrow></mfrac></mrow></mtd><mtd columnalign="center"><mrow><mrow><mi>a</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>p</mi></mrow><mo>;</mo><mi mathvariant="normal">‚ãØ</mi></mrow></mtd></mtr><mtr><mtd columnalign="center"><mn>0</mn></mtd><mtd columnalign="center"><mrow><mrow><mi>a</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>p</mi></mrow><mo>;</mo><mn>1</mn></mrow></mtd><mtd columnalign="center"><mrow><mrow><mi>a</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>p</mi></mrow><mo>;</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>!</mo></mrow></mfrac></mrow></mtd><mtd columnalign="center"><mrow><mrow><mi>a</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>p</mi></mrow><mo>;</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mo>!</mo></mrow></mfrac></mrow></mtd><mtd columnalign="center"><mrow><mrow><mi>a</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>p</mi></mrow><mo>;</mo><mi mathvariant="normal">‚ãØ</mi></mrow></mtd></mtr><mtr><mtd columnalign="center"><mn>0</mn></mtd><mtd columnalign="center"><mrow><mrow><mi>a</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>p</mi></mrow><mo>;</mo><mn>0</mn></mrow></mtd><mtd columnalign="center"><mrow><mrow><mi>a</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>p</mi></mrow><mo>;</mo><mn>1</mn></mrow></mtd><mtd columnalign="center"><mrow><mrow><mi>a</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>p</mi></mrow><mo>;</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>!</mo></mrow></mfrac></mrow></mtd><mtd columnalign="center"><mrow><mrow><mi>a</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>p</mi></mrow><mo>;</mo><mi mathvariant="normal">‚ãØ</mi></mrow></mtd></mtr><mtr><mtd columnalign="center"><mn>0</mn></mtd><mtd columnalign="center"><mrow><mrow><mi>a</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>p</mi></mrow><mo>;</mo><mn>0</mn></mrow></mtd><mtd columnalign="center"><mrow><mrow><mi>a</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>p</mi></mrow><mo>;</mo><mn>0</mn></mrow></mtd><mtd columnalign="center"><mrow><mrow><mi>a</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>p</mi></mrow><mo>;</mo><mn>1</mn></mrow></mtd><mtd columnalign="center"><mrow><mrow><mi>a</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>p</mi></mrow><mo>;</mo><mi mathvariant="normal">‚ãØ</mi></mrow></mtd></mtr><mtr><mtd columnalign="center"><mi></mi></mtd><mtd columnalign="center"><mrow><mrow><mi>a</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>p</mi></mrow><mo>;</mo></mrow></mtd><mtd columnalign="center"><mrow><mrow><mi>a</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>p</mi></mrow><mo>;</mo><mi mathvariant="normal">‚ãØ</mi></mrow></mtd><mtd columnalign="center"><mrow><mrow><mi>a</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>p</mi></mrow><mo>;</mo></mrow></mtd><mtd columnalign="center"><mrow><mrow><mi>a</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>p</mi></mrow><mo>;</mo></mrow></mtd></mtr></mtable><mo>]</mo></mrow></mrow></semantics></math></span></p> <p>where <span class="math-container" id="28231802" visual_id="8201242"><math alttext="D_{kj}=\frac{1}{(j-k)!}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>k</mi><mo>‚Å¢</mo><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>j</mi><mo>-</mo><mi>k</mi></mrow><mo stretchy="false">)</mo></mrow><mo>!</mo></mrow></mfrac></mrow></semantics></math></span> (with negative-argument factorials "naturally" extended as <span class="math-container" id="28231803" visual_id="2076"><math alttext="\infty" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi mathvariant="normal">‚àû</mi></semantics></math></span> so these <span class="math-container" id="28231804" visual_id="524"><math alttext="D" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi></semantics></math></span> terms are <span class="math-container" id="28231805" visual_id="92"><math alttext="0" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0</mn></semantics></math></span>), it shifts up by one row.</p> <p>ADD 4 (sim. time): It looks like we can move both up and down, in particular the matrix <span class="math-container" id="28231806" visual_id="16668"><math alttext="\mathbf{D}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùêÉ</mi></semantics></math></span> with entries <span class="math-container" id="28231807" visual_id="8201243"><math alttext="D_{kj}=(-1)^{k-j}\frac{1}{(k-j)!}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>k</mi><mo>‚Å¢</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><msup><mrow><mo stretchy="false">(</mo><mrow><mo>-</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><mrow><mi>k</mi><mo>-</mo><mi>j</mi></mrow></msup><mo>‚Å¢</mo><mfrac><mn>1</mn><mrow><mrow><mo stretchy="false">(</mo><mrow><mi>k</mi><mo>-</mo><mi>j</mi></mrow><mo stretchy="false">)</mo></mrow><mo>!</mo></mrow></mfrac></mrow></mrow></semantics></math></span> will shift down, while the other matrix above, perhaps better called <span class="math-container" id="28231808" visual_id="34990"><math alttext="\mathbf{U}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùêî</mi></semantics></math></span> instead will shift up. Shifting left and right is possible as well but via the Hadamard product and not the ordinary product, as the second set of relations between columns indicates. In particular, the Hadamard product with the matrix <span class="math-container" id="28231809" visual_id="91914"><math alttext="\mathbf{L}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùêã</mi></semantics></math></span> with entries <span class="math-container" id="28231810" visual_id="8201244"><math alttext="L_{ij}=\frac{i}{j+1}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mrow><mi>i</mi><mo>‚Å¢</mo><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mi>i</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></mfrac></mrow></semantics></math></span> will shift left, and the matrix <span class="math-container" id="28231811" visual_id="5481"><math alttext="\mathbf{R}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùêë</mi></semantics></math></span> with entries <span class="math-container" id="28231812" visual_id="8201245"><math alttext="R_{ij}=\frac{j}{i}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>i</mi><mo>‚Å¢</mo><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mi>j</mi><mi>i</mi></mfrac></mrow></semantics></math></span> will shift right. Thus we have some interesting system for "moving" the matrix about like some kind of tableau or grid - not sure what these symmetry properties do though for easing the analytic solution/explicit formula of the matrixpower as a summation.</p>
</div>
<hr/>
<div id="tags">
<span> real-analysis </span><span> linear-algebra </span><span> combinatorics </span><span> functional-analysis </span><span> tetration </span>
</div>
<hr/>
<div id="question-comments">
<table>
<tbody>
<tr><td comment_id="6286149"> By my numerical tests, the method using truncated Carleman-matrices and try to approach to a sensical result by increasing the size of the matrix, it seems as if we implement an approximation to the Kneser method. The unfortunate thing is that the diagonalizations lead to sets of eigenvalues which vary much with the matrix-size, so for instance use of a 64x64-matrix approximated the Kneser-solution to some error of 1e-6 or so. The advantage(?) of the method is to have a real-to-real approximation for real fractional "heights". One other method (...) </td></tr><tr><td comment_id="6286156"> (...) One other method tries to do the analysis with the ansatz of the Schr√∂der-mechanism which introduces power series for the exponentiation shifted around the attracting or repelling fixpoint. But this works nicely only with bases <span class="math-container" id="28231880" visual_id="368"><math alttext="b" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi></semantics></math></span> from the range <span class="math-container" id="28231814" visual_id="3101009"><math alttext="1\lt b\lt e^{1/e}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>‚Å¢</mo><merror class="ltx_ERROR undefined undefined"><mtext>\lt</mtext></merror><mo>‚Å¢</mo><mi>b</mi><mo>‚Å¢</mo><merror class="ltx_ERROR undefined undefined"><mtext>\lt</mtext></merror><mo>‚Å¢</mo><msup><mi>e</mi><mrow><mn>1</mn><mo>/</mo><mi>e</mi></mrow></msup></mrow></semantics></math></span>, for general bases we get power series/Carlemanmatrices with complex coefficients and difficult to evaluate for fractional heights - and in the general case, complex values even for real fractional heights. I've compared 5 basic methods in a small essay http://go.helms-net.de/math/tetdocs/ComparisionOfInterpolations.pdf </td></tr><tr><td comment_id="6286157"> @Gottfried Helms : Yupp! What I'm trying to do is see if we can't break it down analytically to find a combinatorial formula (infinite summation) for the Kneser tetration, which may enable both its fast and efficient evaluation and its mathematical analysis for interesting properties and relations to other areas of maths. However that's a bit discouraging, since it suggests that a direct attack by diagonalization may not work in the infinite case even if we can fangle some kind of formula. </td></tr><tr><td comment_id="6286159"> The coefficient seem so nice and simple that it feels as if there's almost gotta be some kind of explicit form, but I don't know, it's no proof... (or disproof, either :((( ) </td></tr><tr><td comment_id="6286160"> (...) I've also come across a method proposed by P.Walker and independently by the tetforum-member A. Robbins which use again an ansatz with a finite carleman-matrix and try to find a meaningful approximation by increasing the matrix. However already P. Walker mentioned that the result seems incompatible with the other methods and this was something which I seem to have found independently. </td></tr><tr><td comment_id="6286162"> Schroder equation is not good, because it depends on choice of fixed point and is not real-valued on the real axis. The Kneser (bipolar) solution is what we want. But effectively, solution to Schroder equation is half of the Kneser solution since the latter is a remapping of the former. The trouble is, there are no explicit or even efficient formulae for the latter. </td></tr><tr><td comment_id="6286165"> Yes, the trouble is... . What my observation was, that the Kneser method (as implemented by two members in the tetration-forum) seems to be approximated by the finite-truncation of Carlemanmatrix, when the Carlemanmatrix increases its size, somehow possibly even in a sense of a limit. (difference Kneser - Carleman_nxn for <span class="math-container" id="28231815" visual_id="1477"><math alttext="n\to\infty" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>‚Üí</mo><mi mathvariant="normal">‚àû</mi></mrow></semantics></math></span> goes to zero(?) ). I think I have that observation in the Comparision-article... That reminded me of the two interpolation-methods for the Fibonacci-numbers (complex with Binet-formula, real with cos-formula) </td></tr><tr><td comment_id="6286183"> Last comment at the moment. At the problem of analytical diagonalization of the infinite Carlemanmatrix: I've really invested much time to do this - found an analytical expression via the <span class="math-container" id="28231821" visual_id="364469"><math alttext="\exp(x)-1" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>exp</mi><mo>‚Å°</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>-</mo><mn>1</mn></mrow></semantics></math></span> case and then seeing that this was identical to the Schr√∂der-solution (without knowing it at all and being pointed to it only later). There might be one more possible alternative path here. I've read an article of Eri Jabotinsky who extended the Carlemanmatrix to the twoside infinite index and thus including the <span class="math-container" id="28231817" visual_id="8201246"><math alttext="f(x)^{-k}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>‚Å¢</mo><msup><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mo>-</mo><mi>k</mi></mrow></msup></mrow></semantics></math></span>-case which reminded me of Ramanujan-integrals. But I couldn't get in... </td></tr><tr><td comment_id="6286192"> Right now I'm starting to wonder about the solution through the possibility of establishing an invariant. In particular, I wonder about the possibility of the matrix power as being uniquely determined somehow by preservation of the symmetries evidenced in the downshift/upshift/leftshift/rightshift matrices just given. </td></tr><tr><td comment_id="6286195"> Namely the downshift/upshift matrix is upper triangular, thus it has simple matrixpower. </td></tr><tr><td comment_id="6286198"> Hmm, I did not yet get the downshift/upshift idea. I'd like to delve into this a bit (you might write an answer on your own question containing that ansatz) but it's saturday and much to prepare for christmas. I'll come back to this anyway but more to lurk a bit... </td></tr><tr><td comment_id="6286226"> Another possibility. If the diagonalization is not feasible to find explicitly, maybe there is some other conjugation <span class="math-container" id="28231818" visual_id="8201247"><math alttext="\mathbf{K}(\mathbf{C}[\exp])\mathbf{K}^{-1}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ùêä</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>ùêÇ</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">[</mo><mi>exp</mi><mo stretchy="false">]</mo></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>‚Å¢</mo><msup><mi>ùêä</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></semantics></math></span> that is specific to this matrix, and has a simple matrixpower, </td></tr><tr><td comment_id="6286230"> Of course then there is the question of whether such alternative matrixpower will still yield the desired Kneser tetrational ... </td></tr><tr><td comment_id="6286353"> at "another possibility": yes, it would also be sufficient to find some lower triangular matrix instead of full diagonalization. With infinite-sized matrices it might be possible that multiple solutions are existent, similarly as to the example where the carleman-matrixes for <span class="math-container" id="28231819" visual_id="25487"><math alttext="\log(1+x)" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>log</mi><mo>‚Å°</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><mi>x</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></semantics></math></span>,<span class="math-container" id="28231820" visual_id="8201248"><math alttext="\log(1+x)+2\pi √Æ" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>log</mi><mo>‚Å°</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>+</mo><mi>x</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mn>2</mn><mo>‚Å¢</mo><mi>œÄ</mi><mo>‚Å¢</mo><mi>√Æ</mi></mrow></mrow></semantics></math></span>,... are all left-inverses for the carleman-matrix of <span class="math-container" id="28231821" visual_id="364469"><math alttext="\exp(x)-1" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>exp</mi><mo>‚Å°</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>-</mo><mn>1</mn></mrow></semantics></math></span>. So, perhaps... </td></tr>
</tbody>
</table>
</div>
</div>
<hr style="border-top: 3px double #8c8b8b"/>
</div>
<div class="row">
<div class="answer">
<div answer_id="3049421" id="answer">
<p>Just a short answer what I've found concerning to express analytically the matrix-diagonalization for <span class="math-container" id="28231822" visual_id="8201249"><math alttext="C[exp]" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">[</mo><mrow><mi>e</mi><mo>‚Å¢</mo><mi>x</mi><mo>‚Å¢</mo><mi>p</mi></mrow><mo stretchy="false">]</mo></mrow></mrow></semantics></math></span> which I called <span class="math-container" id="28231823" visual_id="599"><math alttext="B" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi></semantics></math></span> for <span class="math-container" id="28231824" visual_id="49724"><math alttext="\exp(x)" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>exp</mi><mo>‚Å°</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></semantics></math></span> with base <span class="math-container" id="28231825" visual_id="194923"><math alttext="b=e" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo>=</mo><mi>e</mi></mrow></semantics></math></span> and <span class="math-container" id="28231826" visual_id="529185"><math alttext="B_{b}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>B</mi><mi>b</mi></msub></semantics></math></span> for the general <span class="math-container" id="28231827" visual_id="247802"><math alttext="b^{x}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>b</mi><mi>x</mi></msup></semantics></math></span> .<br/> Using the notation <span class="math-container" id="28231828" visual_id="1296807"><math alttext="V(x)=[1,x,x^{2},x^{3},...]" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>V</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo stretchy="false">[</mo><mn>1</mn><mo>,</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi><mn>2</mn></msup><mo>,</mo><msup><mi>x</mi><mn>3</mn></msup><mo>,</mo><mi mathvariant="normal">‚Ä¶</mi><mo stretchy="false">]</mo></mrow></mrow></semantics></math></span> for the infinite "Vandermonde" row-vector of consecutive powers of <span class="math-container" id="28231829" visual_id="67"><math alttext="x" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi></semantics></math></span>, <span class="math-container" id="28231830" visual_id="318"><math alttext="P" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi></semantics></math></span> for the upper-triangular Pascalmatrix, <span class="math-container" id="28231831" visual_id="145185"><math alttext="S2" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>‚Å¢</mo><mn>2</mn></mrow></semantics></math></span> for the lower-triangular matrix of Stirling-numbers <em>2</em>'nd kind, <span class="math-container" id="28231832" visual_id="75"><math alttext="F" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi></semantics></math></span> and <span class="math-container" id="28231833" visual_id="1744268"><math alttext="f=F^{-1}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>=</mo><msup><mi>F</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></semantics></math></span> for the diagonal matrix of factorials and its inverse we can write <span class="math-container" id="28231834" visual_id="8201250"><math alttext="V(x)\cdot B=V(e^{x})" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mrow><mi>V</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>‚ãÖ</mo><mi>B</mi></mrow><mo>=</mo><mrow><mi>V</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><msup><mi>e</mi><mi>x</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow></semantics></math></span> By simple <span class="math-container" id="28231835" visual_id="1135510"><math alttext="LDU" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>‚Å¢</mo><mi>D</mi><mo>‚Å¢</mo><mi>U</mi></mrow></semantics></math></span>-decomposition of <span class="math-container" id="28231836" visual_id="599"><math alttext="B" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi></semantics></math></span> we find <span class="math-container" id="28231837" visual_id="8201251"><math alttext="\begin{array}[]{}&amp;amp;B=(f\cdot S2\cdot F)\cdot P\\ V(x)\cdot((f\cdot S2\cdot F)\cdot P)&amp;amp;=V(e^{x})\\ \end{array}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable columnspacing="5pt" rowspacing="0pt"><mtr><mtd></mtd><mtd columnalign="center"><mrow><mrow><mrow><mi>a</mi><mo>‚Å¢</mo><mi>m</mi><mo>‚Å¢</mo><mi>p</mi></mrow><mo>;</mo><mi>B</mi></mrow><mo>=</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mrow><mrow><mi>f</mi><mo>‚ãÖ</mo><mi>S</mi></mrow><mo>‚Å¢</mo><mn>2</mn></mrow><mo>‚ãÖ</mo><mi>F</mi></mrow><mo stretchy="false">)</mo></mrow><mo>‚ãÖ</mo><mi>P</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign="center"><mrow><mrow><mi>V</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>‚ãÖ</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mrow><mrow><mi>f</mi><mo>‚ãÖ</mo><mi>S</mi></mrow><mo>‚Å¢</mo><mn>2</mn></mrow><mo>‚ãÖ</mo><mi>F</mi></mrow><mo stretchy="false">)</mo></mrow><mo>‚ãÖ</mo><mi>P</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd columnalign="center"><mrow><mi>a</mi><mi>m</mi><mi>p</mi><mo>;</mo><mo>=</mo><mi>V</mi><mrow><mo stretchy="false">(</mo><msup><mi>e</mi><mi>x</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr></mtable></semantics></math></span> What I now found (by numerical tests inspired by some hypothese) was that a pair of suitable powers of <span class="math-container" id="28231838" visual_id="318"><math alttext="P" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi></semantics></math></span> would triangularize <span class="math-container" id="28231839" visual_id="599"><math alttext="B" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi></semantics></math></span>: <span class="math-container" id="28231840" visual_id="8201252"><math alttext="\begin{array}[]{}B&amp;amp;=P^{-t}\cdot U_{t}\cdot P^{t}\\ \end{array}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable columnspacing="5pt"><mtr><mtd columnalign="center"><mi>B</mi></mtd><mtd columnalign="center"><mrow><mi>a</mi><mi>m</mi><mi>p</mi><mo>;</mo><mo>=</mo><msup><mi>P</mi><mrow><mo>-</mo><mi>t</mi></mrow></msup><mo>‚ãÖ</mo><msub><mi>U</mi><mi>t</mi></msub><mo>‚ãÖ</mo><msup><mi>P</mi><mi>t</mi></msup></mrow></mtd></mtr></mtable></semantics></math></span> where <span class="math-container" id="28231841" visual_id="297"><math alttext="t" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi></semantics></math></span> is the (complex) fixpoint of <span class="math-container" id="28231842" visual_id="181702"><math alttext="\exp()" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>exp</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mo stretchy="false">)</mo></mrow></mrow></semantics></math></span> and <span class="math-container" id="28231843" visual_id="165405"><math alttext="U_{t}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>U</mi><mi>t</mi></msub></semantics></math></span> was lower triangular. Let's write <span class="math-container" id="28231844" visual_id="506"><math alttext="u" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi></semantics></math></span> for <span class="math-container" id="28231845" visual_id="85310"><math alttext="u=\log(t)" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>=</mo><mrow><mi>log</mi><mo>‚Å°</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></semantics></math></span> and <span class="math-container" id="28231846" visual_id="3778209"><math alttext="\;{}^{d}V(\cdot)" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mpadded lspace="2.8pt" width="+2.8pt"><mmultiscripts><mi>V</mi><mprescripts></mprescripts><none></none><mi>d</mi></mmultiscripts></mpadded><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mo>‚ãÖ</mo><mo stretchy="false">)</mo></mrow></mrow></semantics></math></span> for the diagonally written <span class="math-container" id="28231847" visual_id="3123"><math alttext="V(\cdot)" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mo>‚ãÖ</mo><mo stretchy="false">)</mo></mrow></mrow></semantics></math></span>. Then this is simply  <span class="math-container" id="28231848" visual_id="8201253"><math alttext="U_{t}=\;^{d}V(u)\cdot fS2F" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>U</mi><mi>t</mi></msub><msup><mo rspace="5.3pt">=</mo><mi>d</mi></msup><mrow><mrow><mrow><mi>V</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mi>u</mi><mo stretchy="false">)</mo></mrow></mrow><mo>‚ãÖ</mo><mi>f</mi></mrow><mo>‚Å¢</mo><mi>S</mi><mo>‚Å¢</mo><mn>2</mn><mo>‚Å¢</mo><mi>F</mi></mrow></mrow></semantics></math></span> This can be diagonalized (but with complex coefficients if <span class="math-container" id="28231849" visual_id="297"><math alttext="t" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi></semantics></math></span> or more precisely <span class="math-container" id="28231850" visual_id="8201254"><math alttext="u(=\log(t))" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mspace width="veryverythickmathspace"></mspace><mrow><mo stretchy="false">(</mo><mrow><mi></mi><mo>=</mo><mrow><mi>log</mi><mo>‚Å°</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></semantics></math></span> is complex as in our current example) <span class="math-container" id="28231851" visual_id="8201255"><math alttext="\begin{array}[]{}U_{t}&amp;amp;=W_{t}\cdot\;^{d}V(u)\cdot W_{t}^{-1}\\ \end{array}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable columnspacing="5pt"><mtr><mtd columnalign="center"><msub><mi>U</mi><mi>t</mi></msub></mtd><mtd columnalign="center"><mrow><mi>a</mi><mi>m</mi><mi>p</mi><mo>;</mo><mo>=</mo><msub><mi>W</mi><mi>t</mi></msub><msup><mo rspace="5.3pt">‚ãÖ</mo><mi>d</mi></msup><mi>V</mi><mrow><mo stretchy="false">(</mo><mi>u</mi><mo stretchy="false">)</mo></mrow><mo>‚ãÖ</mo><msubsup><mi>W</mi><mi>t</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></mtd></mtr></mtable></semantics></math></span> So all in all we could write <span class="math-container" id="28231852" visual_id="8201256"><math alttext="\begin{array}[]{}V(x)\cdot(P^{-t}\cdot(W_{t}\cdot\;^{d}V(u)\cdot W_{t}^{-1})% \cdot P^{t})&amp;amp;=V(e^{x})\\ V(x)\cdot(P^{-t}\cdot(W_{t}\cdot\;^{d}V(u^{h})\cdot W_{t}^{-1})\cdot P^{t})&amp;% amp;=V(\exp¬∞^{h}(x))\\ \text{ and conveniantly }\\ (V(x)\cdot P^{-t})\cdot(W_{t}\cdot\;^{d}V(u^{h})\cdot W_{t}^{-1})&amp;amp;=V(\exp¬∞% ^{h}(x))\cdot P^{-t}\\ \end{array}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable columnspacing="5pt" rowspacing="0pt"><mtr><mtd columnalign="center"><mrow><mrow><mi>V</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>‚ãÖ</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>P</mi><mrow><mo>-</mo><mi>t</mi></mrow></msup><mo>‚ãÖ</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mrow><msub><mi>W</mi><mi>t</mi></msub><msup><mo rspace="5.3pt">‚ãÖ</mo><mi>d</mi></msup><mi>V</mi></mrow><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mi>u</mi><mo stretchy="false">)</mo></mrow></mrow><mo>‚ãÖ</mo><msubsup><mi>W</mi><mi>t</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow><mo stretchy="false">)</mo></mrow><mo>‚ãÖ</mo><msup><mi>P</mi><mi>t</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd columnalign="center"><mrow><mi>a</mi><mi>m</mi><mi>p</mi><mo>;</mo><mo>=</mo><mi>V</mi><mrow><mo stretchy="false">(</mo><msup><mi>e</mi><mi>x</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign="center"><mrow><mrow><mi>V</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>‚ãÖ</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>P</mi><mrow><mo>-</mo><mi>t</mi></mrow></msup><mo>‚ãÖ</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mrow><msub><mi>W</mi><mi>t</mi></msub><msup><mo rspace="5.3pt">‚ãÖ</mo><mi>d</mi></msup><mi>V</mi></mrow><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><msup><mi>u</mi><mi>h</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>‚ãÖ</mo><msubsup><mi>W</mi><mi>t</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow><mo stretchy="false">)</mo></mrow><mo>‚ãÖ</mo><msup><mi>P</mi><mi>t</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd columnalign="center"><mrow><mi>a</mi><mi>m</mi><mi>p</mi><mo>;</mo><mo>=</mo><mi>V</mi><mrow><mo stretchy="false">(</mo><mi>exp</mi><msup><mi mathvariant="normal">¬∞</mi><mi>h</mi></msup><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign="center"><mtext>¬†and conveniantly¬†</mtext></mtd></mtr><mtr><mtd columnalign="center"><mrow><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>V</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>‚ãÖ</mo><msup><mi>P</mi><mrow><mo>-</mo><mi>t</mi></mrow></msup></mrow><mo stretchy="false">)</mo></mrow><mo>‚ãÖ</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mrow><msub><mi>W</mi><mi>t</mi></msub><msup><mo rspace="5.3pt">‚ãÖ</mo><mi>d</mi></msup><mi>V</mi></mrow><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><msup><mi>u</mi><mi>h</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>‚ãÖ</mo><msubsup><mi>W</mi><mi>t</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd columnalign="center"><mrow><mi>a</mi><mi>m</mi><mi>p</mi><mo>;</mo><mo>=</mo><mi>V</mi><mrow><mo stretchy="false">(</mo><mi>exp</mi><msup><mi mathvariant="normal">¬∞</mi><mi>h</mi></msup><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mo>‚ãÖ</mo><msup><mi>P</mi><mrow><mo>-</mo><mi>t</mi></mrow></msup></mrow></mtd></mtr></mtable></semantics></math></span> Ironically, by the functionality of <span class="math-container" id="28231853" visual_id="318"><math alttext="P" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi></semantics></math></span> and its powers <span class="math-container" id="28231854" visual_id="8201257"><math alttext="P^{-t}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>P</mi><mrow><mo>-</mo><mi>t</mi></mrow></msup></semantics></math></span> we have <span class="math-container" id="28231855" visual_id="8201258"><math alttext="V(x)\cdot P^{-t}=V(x-t)" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mrow><mi>V</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>‚ãÖ</mo><msup><mi>P</mi><mrow><mo>-</mo><mi>t</mi></mrow></msup></mrow><mo>=</mo><mrow><mi>V</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>x</mi><mo>-</mo><mi>t</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></semantics></math></span> and our formula reduces by further cosmetic to <span class="math-container" id="28231856" visual_id="8201259"><math alttext="\begin{array}[]{}V(x-t)\cdot(W_{t}\cdot\;^{d}V(u)\cdot W_{t}^{-1})&amp;amp;=V(e^{x% }-t)\\ \end{array}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable columnspacing="5pt"><mtr><mtd columnalign="center"><mrow><mrow><mi>V</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>x</mi><mo>-</mo><mi>t</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>‚ãÖ</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mrow><msub><mi>W</mi><mi>t</mi></msub><msup><mo rspace="5.3pt">‚ãÖ</mo><mi>d</mi></msup><mi>V</mi></mrow><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mi>u</mi><mo stretchy="false">)</mo></mrow></mrow><mo>‚ãÖ</mo><msubsup><mi>W</mi><mi>t</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow></mtd><mtd columnalign="center"><mrow><mi>a</mi><mi>m</mi><mi>p</mi><mo>;</mo><mo>=</mo><mi>V</mi><mrow><mo stretchy="false">(</mo><msup><mi>e</mi><mi>x</mi></msup><mo>-</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mrow></mtd></mtr></mtable></semantics></math></span> and one recognizes the mechanism of shift of the powerseries towards its fixpoint <span class="math-container" id="28231857" visual_id="297"><math alttext="t" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi></semantics></math></span>! It needed not much time to recognize that <span class="math-container" id="28231858" visual_id="12583"><math alttext="W_{t}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>W</mi><mi>t</mi></msub></semantics></math></span> is just the Carlemanmatrix for the Schr√∂der-function, having complex coefficients (in our current example), thus this all gives complex resulting values for fractional real heights on real startingvalues <span class="math-container" id="28231859" visual_id="67"><math alttext="x" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi></semantics></math></span>.<br/> <hr/> This all is simpler (and has only real numbers) if the fixpoint <span class="math-container" id="28231860" visual_id="297"><math alttext="t" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi></semantics></math></span> is real, and so the base <span class="math-container" id="28231861" visual_id="368"><math alttext="b" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi></semantics></math></span> is <span class="math-container" id="28231862" visual_id="3101009"><math alttext="1\lt b\lt e^{1/e}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>‚Å¢</mo><merror class="ltx_ERROR undefined undefined"><mtext>\lt</mtext></merror><mo>‚Å¢</mo><mi>b</mi><mo>‚Å¢</mo><merror class="ltx_ERROR undefined undefined"><mtext>\lt</mtext></merror><mo>‚Å¢</mo><msup><mi>e</mi><mrow><mn>1</mn><mo>/</mo><mi>e</mi></mrow></msup></mrow></semantics></math></span> thus <span class="math-container" id="28231863" visual_id="8201260"><math alttext="1\lt t\lt e" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>‚Å¢</mo><merror class="ltx_ERROR undefined undefined"><mtext>\lt</mtext></merror><mo>‚Å¢</mo><mi>t</mi><mo>‚Å¢</mo><merror class="ltx_ERROR undefined undefined"><mtext>\lt</mtext></merror><mo>‚Å¢</mo><mi>e</mi></mrow></semantics></math></span> and <span class="math-container" id="28231864" visual_id="8201261"><math alttext="0\lt u(=\log(t))\lt 1" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><merror class="ltx_ERROR undefined undefined"><mtext>\lt</mtext></merror><mi>u</mi><mrow><mo stretchy="false">(</mo><mo>=</mo><mi>log</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><merror class="ltx_ERROR undefined undefined"><mtext>\lt</mtext></merror><mn>1</mn></mrow></semantics></math></span>. For instance, if <span class="math-container" id="28231865" visual_id="272906"><math alttext="b=\sqrt{2}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo>=</mo><msqrt><mn>2</mn></msqrt></mrow></semantics></math></span> then <span class="math-container" id="28231866" visual_id="27273"><math alttext="t=2" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mn>2</mn></mrow></semantics></math></span> and <span class="math-container" id="28231867" visual_id="8201262"><math alttext="u=\log(2)\approx 0.693" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>=</mo><mrow><mi>log</mi><mo>‚Å°</mo><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></mrow><mo>‚âà</mo><mn>0.693</mn></mrow></semantics></math></span> then also <span class="math-container" id="28231868" visual_id="12583"><math alttext="W_{t}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>W</mi><mi>t</mi></msub></semantics></math></span> has only real coefficients.  Moreover, all coefficients in <span class="math-container" id="28231869" visual_id="8201263"><math alttext="U_{t}=W_{t}\cdot\;^{d}V(u^{h})\cdot W_{t}^{-1}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>U</mi><mi>t</mi></msub><mo>=</mo><mrow><mrow><mrow><msub><mi>W</mi><mi>t</mi></msub><msup><mo rspace="5.3pt">‚ãÖ</mo><mi>d</mi></msup><mi>V</mi></mrow><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><msup><mi>u</mi><mi>h</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo>‚ãÖ</mo><msubsup><mi>W</mi><mi>t</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></mrow></semantics></math></span> (where <span class="math-container" id="28231870" visual_id="702"><math alttext="h" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi></semantics></math></span> is the iteration-height, possibly fractional) are real and are descibable in terms of polynomials in <span class="math-container" id="28231871" visual_id="506"><math alttext="u" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi></semantics></math></span> and <span class="math-container" id="28231872" visual_id="702"><math alttext="h" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi></semantics></math></span>, with the order depending of the index in the powerseries (see <a href="http://go.helms-net.de/math/tetdocs/ContinuousfunctionalIteration.pdf" rel="nofollow noreferrer">text</a>, see <a href="http://go.helms-net.de/math/tetdocs/" rel="nofollow noreferrer">index-of-pages</a>)<br/> Additional remark: W. Reshetnikow in mathoverflow presented this year(don't have the link at hand) an ansatz which avoided the Carleman-matrix-method introducing essentially the <span class="math-container" id="28231873" visual_id="625"><math alttext="q" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>q</mi></semantics></math></span>-factorials and -binomials. This made it really good looking! However, by my own earlier analyses I had found out that this leads to the same polynomials in <span class="math-container" id="28231874" visual_id="506"><math alttext="u" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi></semantics></math></span> and <span class="math-container" id="28231875" visual_id="702"><math alttext="h" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi></semantics></math></span> as I had found them via the just described analysis of the infinite Carlemanmatrices.</p> <p>So the <em>analytical</em> access via <em>infinite</em> Carlemanmatrices and their diagonalization leads naturally to the Schr√∂der-function and the Schr√∂der-mechanism (including shift towards fixpoints) - which is, what you did not want...  </p>
</div>
<hr/>
<div id="answer-comments">
<table>
<tbody>
<tr><td comment_id="6286357"> Hmm. This is interesting, as it suggests that the matrixpower is not unique, as thought. What you really want is then a matrixpower which ends up as somehow the limit of the finite matrixpowers for truncated matrix. </td></tr><tr><td comment_id="6286359"> @The_Sympathizer : hmm, only to avoid a possible misunderstanding: what/where do you mean that refers to "matrix-power is not unique"? </td></tr><tr><td comment_id="6286364"> It seems (and I might have read it wrong) that you constructed a sort of matrixpower which causes the Carleman matrix to power like the Schroder equation solution. </td></tr><tr><td comment_id="6286367"> Though now I'm not sure... </td></tr><tr><td comment_id="6286370"> Yes, it "implements" exactly the Schr√∂der-mechanism, and so the fractional powers of matrix and by this the fractional iterates of function. Note that the eigenmatrix <span class="math-container" id="28231876" visual_id="304"><math alttext="W" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi></semantics></math></span> can in general be understood as (formal) infinite power of the basic matrix-operator for the function and this reflects then as well the term of the infinite iterated basic-function in the Schr√∂der-mechanism. So <span class="math-container" id="28231877" visual_id="12583"><math alttext="W_{t}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>W</mi><mi>t</mi></msub></semantics></math></span> is the Carlemanmatrix of the Schr√∂der-function <span class="math-container" id="28231878" visual_id="85347"><math alttext="\sigma()" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÉ</mi><mo>‚Å¢</mo><mrow><mo stretchy="false">(</mo><mo stretchy="false">)</mo></mrow></mrow></semantics></math></span> </td></tr><tr><td comment_id="6286378"> ... but note, this is only true if we work analytically on the ***infinite***-sized Carlemanmatrix. Beginning with the ***finitely*** truncated Carlemanmatrix (without fixpoint-shift) and then hoping to approximate the true mechanism for tetration by increasing the size of the truncation does ***not*** approach the results here, but, as I tend to assume, the real-to-real-solution of H. Kneser (in the implementation of the tetforum-members in Pari/GP). We had a short controverse about this in the tetforum between Henryk and me but my knowledge was then only small and so it was left unresolved. </td></tr><tr><td comment_id="6286402"> Thought I'd add another point here and that's that I just started playing around with the eigenvalues, because you mentioned how they seemed not to approach anything and just blow up. I realized this may be wrong, because one thing you have to remember is eigenvalues do not have a natural order; in particular, you can rearrange them in the diagonalization provided you also rearrange the eigenvectors likewise. Thus one should not take seeming chaos in them in terms of the direct output from an eigenvalue solver as being necessarily significant. </td></tr><tr><td comment_id="6286404"> Instead you need to look at them wholistically, as an unordered set, e.g. as plotting by a scatter plot. When you do this, it seems that patterns begin to emerge. </td></tr><tr><td comment_id="6286429"> Thei eigenvalues by the Schr√∂der-machanism have a simple pattern: just the sequence of powers of <span class="math-container" id="28231879" visual_id="506"><math alttext="u" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi></semantics></math></span>. The problematic case is that of the "truncating" approach. If base <span class="math-container" id="28231880" visual_id="368"><math alttext="b" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi></semantics></math></span> is in <span class="math-container" id="28231881" visual_id="3101009"><math alttext="1\lt b\lt e^{1/e}" class="ltx_Math" display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>‚Å¢</mo><merror class="ltx_ERROR undefined undefined"><mtext>\lt</mtext></merror><mo>‚Å¢</mo><mi>b</mi><mo>‚Å¢</mo><merror class="ltx_ERROR undefined undefined"><mtext>\lt</mtext></merror><mo>‚Å¢</mo><msup><mi>e</mi><mrow><mn>1</mn><mo>/</mo><mi>e</mi></mrow></msup></mrow></semantics></math></span> then the pattern approximates the Schr√∂der-eigenvalues when you increase the size of truncation. But for the other cases the pattern is only in the middle segment approximating such a scheme. </td></tr>
</tbody>
</table>
</div>
</div>
<hr style="border-top: 3px double #8c8b8b"/>
</div>
<div class="row">
<div id="duplicate">
<table>
<tbody>
</tbody>
</table>
</div>
<hr/>
<div id="related">
<table>
<tbody>
<tr><td post_id="2235730"> Diagonalization of an infinite matrix </td></tr>
</tbody>
</table>
</div>
</div>
</div>
</body>
</html>
